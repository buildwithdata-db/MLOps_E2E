# Databricks notebook source
# MAGIC %md
# MAGIC ## Churn Prediction Batch Inference
# MAGIC 
# MAGIC <img src="https://github.com/RafiKurlansik/laughing-garbanzo/blob/main/step6.png?raw=true">

# COMMAND ----------

# MAGIC %md
# MAGIC #### Load Model
# MAGIC 
# MAGIC Loading as a Spark UDF to set us up for future scale.

# COMMAND ----------

# MAGIC %run ./Shared_Include

# COMMAND ----------

# MAGIC %md ###Create or load an Azure ML Workspace
# MAGIC 
# MAGIC Before models can be deployed to Azure ML, you must create or obtain an Azure ML Workspace. The azureml.core.Workspace.create() function will load a workspace of a specified name or create one if it does not already exist. For more information about creating an Azure ML Workspace, see the Azure ML Workspace management documentation.

# COMMAND ----------

# MAGIC %pip install azureml-sdk

# COMMAND ----------

import azureml
from azureml.core import Workspace
 
workspace_name = "wks-mlops"
workspace_location="eastus2"
resource_group = "databricks-mlops-demo"
subscription_id = "6369c148-f8a9-4fb5-8a9d-ac1b2c8e756e"
 
workspace = Workspace.create(name = workspace_name,
                             location = workspace_location,
                             resource_group = resource_group,
                             subscription_id = subscription_id,
                             exist_ok=True)

# COMMAND ----------

run_id1 = "1f18894185cd47b5ac3924c2e8f8a790"
model_uri = "runs:/" + run_id1 + "/model"

# COMMAND ----------

import mlflow
from azureml.core import Workspace

ws = Workspace.from_config()

mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())

# COMMAND ----------

import mlflow.azureml

model_image, azure_model = mlflow.azureml.build_image(model_uri=model_uri, 
                                                      workspace=workspace,
                                                      model_name="churn_model",
                                                      image_name="img_churn_model",
                                                      description="churn model demo",
                                                      synchronous=False)

# COMMAND ----------

workspace.get_details()

# COMMAND ----------

from azureml.core.compute import AksCompute, ComputeTarget
 
# Use the default configuration (you can also provide parameters to customize this)
prov_config = AksCompute.provisioning_configuration()
 
aks_cluster_name = "aksmlops-cluster" 
# Create the cluster
aks_target = ComputeTarget.create(workspace = workspace, 
                                  name = aks_cluster_name, 
                                  provisioning_configuration = prov_config)
 
# Wait for the create process to complete
aks_target.wait_for_completion(show_output = True)
print(aks_target.provisioning_state)
print(aks_target.provisioning_errors)

# COMMAND ----------

import json
  
# Data to be written
deploy_config = {
  "computeType": "aks", "computeTargetName": "aksmlops-cluster"
}

# Serializing json 
json_object = json.dumps(deploy_config)
  
# # Writing to sample.json
# with open("dbfs:/FileStore/leomao/deployment_config.json", "w") as outfile:
#     outfile.write(json_object)

# COMMAND ----------

{"computeType": "aks", "computeTargetName": "aksmlops-cluster41102437"}

# COMMAND ----------

# MAGIC %fs
# MAGIC 
# MAGIC ls "dbfs:/FileStore/leomao"

# COMMAND ----------

from mlflow.deployments import get_deploy_client

# set the tracking uri as the deployment client
client = get_deploy_client(mlflow.get_tracking_uri())


# COMMAND ----------

from mlflow.deployments import get_deploy_client

# set the tracking uri as the deployment client
client = get_deploy_client(mlflow.get_tracking_uri())

# set the model path 
model_path = "model"

# set the deployment config
deploy_path = "dbfs:/FileStore/leomao/deployment_config.json"
test_config = {'deploy-config-file': deploy_path}

# define the model path and the name is the service name
# the model gets registered automatically and a name is autogenerated using the "name" parameter below 
client.create_deployment(model_uri='runs:/{}/{}'.format(run.id, model_path),
                         config=test_config,
                         name="mlflow-test-aci")

# COMMAND ----------

from azureml.core.compute import AksCompute, ComputeTarget
 
# Get the resource group from https://porta..azure.com -> Find your resource group
resource_group = "databricks-mlops-demo"
 
# Give the cluster a local name
aks_cluster_name = "aksmlops"
 
# Attatch the cluster to your workgroup
attach_config = AksCompute.attach_configuration(resource_group=resource_group, cluster_name=aks_cluster_name)
aks_target = ComputeTarget.attach(workspace, name="diabetes-compute", attach_config)
 
# Wait for the operation to complete
aks_target.wait_for_completion(True)
print(aks_target.provisioning_state)
print(aks_target.provisioning_errors)

# COMMAND ----------

# import mlflow
# model = mlflow.pyfunc.spark_udf(spark, model_uri=f"models:/{churn_model_name}/staging") # may need to replace with your own model name

# COMMAND ----------

# MAGIC %md
# MAGIC #### Load Features

# COMMAND ----------

from databricks.feature_store import FeatureStoreClient

fs = FeatureStoreClient()
features = fs.read_table(f'{database_name}.churn_features')

# COMMAND ----------

sample = features.toPandas()
# spark_df = createDataFrame(pandas_df)

# COMMAND ----------

type(sample.iloc[[0]])

# COMMAND ----------

# MAGIC %md
# MAGIC #### Online Inference

# COMMAND ----------

scoring_uri = 'https://adb-984752964297111.11.azuredatabricks.net/model/leo_mao_churn_demo/1/invocations'

# COMMAND ----------

import os
import json
token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
# prediction = query_endpoint_example(scoring_uri=scoring_uri, inputs=json.loads(sample_json), service_key=token)

# COMMAND ----------

import os
import requests
import numpy as np
import pandas as pd

def create_tf_serving_json(data):
  return {'inputs': {name: data[name].tolist() for name in data.keys()} if isinstance(data, dict) else data.tolist()}

def score_model(dataset):
  headers = {'Authorization': f'Bearer {token}'}
  data_json = dataset.to_dict(orient='split') if isinstance(dataset, pd.DataFrame) else create_tf_serving_json(dataset)
  response = requests.request(method='POST', headers=headers, url=scoring_uri, json=data_json)
  if response.status_code != 200:
    raise Exception(f'Request failed with status {response.status_code}, {response.text}')
  return response.json()

# COMMAND ----------

# MAGIC %sh
# MAGIC 
# MAGIC ls /Files

# COMMAND ----------

# MAGIC %sh 
# MAGIC 
# MAGIC curl \
# MAGIC   -u token:$DATABRICKS_TOKEN \
# MAGIC   -X POST \
# MAGIC   -H "Content-Type: application/json; format=pandas-records" \
# MAGIC   -d@data.json \
# MAGIC   https://adb-984752964297111.11.azuredatabricks.net/model/leo_mao_churn_demo/1/invocations

# COMMAND ----------

result1 = score_model(sample.iloc[[0]])
print(f"prediction={result1}")

# COMMAND ----------

data = pd.DataFrame([
  {
    "customerID": "6849-OYAMU",
    "seniorCitizen": 0,
    "tenure": 19,
    "monthlyCharges": 100,
    "totalCharges": 1888.65,
    "gender_Female": 0,
    "gender_Male": 1,
    "partner_No": 0,
    "partner_Yes": 1,
    "dependents_No": 0,
    "dependents_Yes": 1,
    "phoneService_No": 0,
    "phoneService_Yes": 1,
    "multipleLines_No": 1,
    "multipleLines_Nophoneservice": 0,
    "multipleLines_Yes": 0,
    "internetService_DSL": 0,
    "internetService_Fiberoptic": 1,
    "internetService_No": 0,
    "onlineSecurity_No": 1,
    "onlineSecurity_Nointernetservice": 0,
    "onlineSecurity_Yes": 0,
    "onlineBackup_No": 0,
    "onlineBackup_Nointernetservice": 0,
    "onlineBackup_Yes": 1,
    "deviceProtection_No": 1,
    "deviceProtection_Nointernetservice": 0,
    "deviceProtection_Yes": 0,
    "techSupport_No": 0,
    "techSupport_Nointernetservice": 0,
    "techSupport_Yes": 1,
    "streamingTV_No": 0,
    "streamingTV_Nointernetservice": 0,
    "streamingTV_Yes": 1,
    "streamingMovies_No": 0,
    "streamingMovies_Nointernetservice": 0,
    "streamingMovies_Yes": 1,
    "contract_Month-to-month": 0,
    "contract_Oneyear": 1,
    "contract_Twoyear": 0,
    "paperlessBilling_No": 1,
    "paperlessBilling_Yes": 0,
    "paymentMethod_Banktransfer-automatic": 1,
    "paymentMethod_Creditcard-automatic": 0,
    "paymentMethod_Electroniccheck": 0,
    "paymentMethod_Mailedcheck": 0
  }
])

# COMMAND ----------

{
  "customerID": "6849-OYAMU",
  "seniorCitizen": 0,
  "tenure": 19,
  "monthlyCharges": 100,
  "totalCharges": 1888.65,
  "gender_Female": 0,
  "gender_Male": 1,
  "partner_No": 0,
  "partner_Yes": 1,
  "dependents_No": 0,
  "dependents_Yes": 1,
  "phoneService_No": 0,
  "phoneService_Yes": 1,
  "multipleLines_No": 1,
  "multipleLines_Nophoneservice": 0,
  "multipleLines_Yes": 0,
  "internetService_DSL": 0,
  "internetService_Fiberoptic": 1,
  "internetService_No": 0,
  "onlineSecurity_No": 1,
  "onlineSecurity_Nointernetservice": 0,
  "onlineSecurity_Yes": 0,
  "onlineBackup_No": 0,
  "onlineBackup_Nointernetservice": 0,
  "onlineBackup_Yes": 1,
  "deviceProtection_No": 1,
  "deviceProtection_Nointernetservice": 0,
  "deviceProtection_Yes": 0,
  "techSupport_No": 0,
  "techSupport_Nointernetservice": 0,
  "techSupport_Yes": 1,
  "streamingTV_No": 0,
  "streamingTV_Nointernetservice": 0,
  "streamingTV_Yes": 1,
  "streamingMovies_No": 0,
  "streamingMovies_Nointernetservice": 0,
  "streamingMovies_Yes": 1,
  "contract_Month-to-month": 0,
  "contract_Oneyear": 1,
  "contract_Twoyear": 0,
  "paperlessBilling_No": 1,
  "paperlessBilling_Yes": 0,
  "paymentMethod_Banktransfer-automatic": 1,
  "paymentMethod_Creditcard-automatic": 0,
  "paymentMethod_Electroniccheck": 0,
  "paymentMethod_Mailedcheck": 0
}

# COMMAND ----------

result2 = score_model(data)
print(f"prediction={result2}")
